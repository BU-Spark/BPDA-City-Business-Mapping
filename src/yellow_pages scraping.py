#-------------------------------------------------------------------------------------------# -- DATE: June 2019# -- PURPOSE: Scrape locations from Yellow Pages based on zipcode & search term.#-------------------------------------------------------------------------------------------#-------------------------------------------------------------------------------------------# IMPORT STATEMENTS#-------------------------------------------------------------------------------------------from bs4 import BeautifulSoup as bs     # parsingfrom urllib.request import urlopen      # scrappingimport http.clientimport pandas as pd                     # exportingfrom tenacity import retry, stop_after_attempt@retry(stop=stop_after_attempt(6))def retry_url(url):    try:        return urlopen(url)    except (http.client.IncompleteRead) as e:                return e.partial    #-------------------------------------------------------------------------------------------# this function loops through all of the available pages on Yellow Pagesdef pg_loop(locationsAll, startPage, searchURL):    # There is not a clear way to know when pagination is finished; currently the yellowpages     # website allows the url for page numbers without results.    if startPage == 1:        # Each full page returns 30; thus we know to finish looping through pages when the        # results are under 30.        pageLength = 30    else:        exit()        # See note above    while startPage <= 101:        pgURL = searchURL + '&page={}'.format(startPage)        print(pgURL)        try:            yp_html = retry_url(pgURL)            #yp_html.raise_for_status()            soup = bs(yp_html, "html.parser")            if str(soup.find('a')) != 'None':                locationInfo = soup.findAll('div', {'class': ['info']})                pageLength = len(locationInfo)                # This loops through each locations on the page and parses out the information                 # we care about.                for element in locationInfo:                    if element.find('span', {'class': 'ad-pill'}) == None:                        rec = element_parser(element, startPage)                        # Here we de-dup the list to make sure we aren't adding items that already                        # exist.                        if rec not in locationsAll:                            locationsAll.append(rec)            startPage += 1        except Exception as e:            print(e)            return        # -------------------------------------------------------------------------------------------                def element_parser(element, pg):    # Extract the categories    categoriesPrepped = []    if element.find('div', {'class': 'categories'}) != None:        locationCategories = element.find('div', {'class': 'categories'}).findAll('a')        for category in locationCategories:            categoriesPrepped.append(category.string)    else:        categoriesPrepped = []        # Extract the location name    if element.find('a', {'class': 'business-name'}) != None:        locationName = element.find('a', {'class': 'business-name'}).string    else:        locationName = ''            # Extract the location address    if element.p != None:        # Address field         if element.p.find('div', {'class': 'street-address'}) != None:            locationAddress = element.p.find('div', {'class': 'street-address'}).string        elif element.p.find('span', {'class': 'street-address'}) != None:            locationAddress = element.p.find('span', {'class': 'street-address'}).string        else:            locationAddress = ''                # This is the only portion I worry about at scale due to using the string function         # and the order of the elements instead of the element names. In the end, I didn't        # really see a way around this due to the lack of differential in the items.        # City, State, Zip        if element.p.find('div', {'class':'locality'}) != None:            locationCity = element.p.find('div', {'class':'locality'}).string.split(', ')[0]            locationState = element.p.find('div', {'class':'locality'}).string.split(', ')[1].split(' ')[0]            locationZip = element.p.find('div', {'class':'locality'}).string.split(', ')[1].split(' ')[1]        elif element.p.find('span', {'class':'locality'}) != None:            locationCity = element.p.find('span', {'class':'locality'}).string.split(',')[0]            locationState = element.p.findAll('span', {'class':''})[0].string            locationZip = element.p.findAll('span', {'class':''})[1].string           else:            locationCity = ''            locationState = ''            locationZip = ''    else:        locationAddress = ''        locationCity = ''        locationState = ''        locationZip = ''             # Extract phones                        if element.find('div', {'class':'phones phone primary'}) != None:        locationPhone = element.find('div', {'class':'phones phone primary'}).string    else:        locationPhone = ''    # Extract extra informaiton    baseURL = "https://www.yellowpages.com"        link = element.find('a', {'class': 'business-name'}, href=True)        url = baseURL + link['href']        html = retry_url(url)        bsoup = bs(html, "html.parser")        # Extract opening time        if len(bsoup.findAll('div', {'class': ['time-info']})) > 0:                timeInfo = bsoup.findAll('div', {'class': ['time-info']})                for elem in timeInfo:            openInfo = elem.findAll('div', {'class': None})            if len(openInfo) == 2:                todayTime = openInfo[0].string                tomorrowTime = openInfo[1].string                if 'Closed' not in todayTime:                    openTime = todayTime.split('Today: ')[1]                else:                    openTime = tomorrowTime.split('Tomorrow: ')[1]            else:                time = openInfo[0].string                                    if 'Today' in time:                    openTime = time.split('Today: ')[1]                else:                    openTime = time.split('Tomorrow: ')[1]    else:         openTime = ''    # Extract years in business    if bsoup.find('div', {'class': 'number'}) != None:        openYears = bsoup.find('div', {'class': 'number'}).string    else:        openYears = ''                rec = {        'locationName': locationName,        'categories': categoriesPrepped,        'locationAddress': locationAddress,        'locationCity': locationCity,        'locationState': locationState,        'locationZip': locationZip,        'locationPhone': locationPhone,        'openTime': openTime,        'Years In Business': openYears        }            return rec# -------------------------------------------------------------------------------------------if __name__ == "__main__":    # The function element_parser will append all locations to this list.    locationsAll = []      baseURL = "https://www.yellowpages.com"    searchURL = baseURL + "/search?search_terms=restaurants&geo_location_terms=Allston%2C%20MA"    startPage = 1    finalLocations = pg_loop(locationsAll, startPage, searchURL)    # I personally like writing an entire dataframe to a csv instead of using the    # csv library to write each row. I find it is faster and cleaner. Eventually    # we will change this to an db insert function.    locationsAllDF = pd.DataFrame(locationsAll)    locationsAllDF.to_csv('~/Desktop/yp_results.csv')